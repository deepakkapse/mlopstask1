{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.910394e-12 0.000000e+00 1.000000e+00]]\n",
      "[[9.9998331e-01 0.0000000e+00 1.6638085e-05]]\n",
      "[[2.107957e-09 0.000000e+00 1.000000e+00]]\n",
      "[[1.0000000e+00 0.0000000e+00 3.1974236e-16]]\n",
      "[[9.992562e-01 0.000000e+00 7.438191e-04]]\n",
      "[[0.97952646 0.         0.02047355]]\n",
      "[[6.725619e-26 0.000000e+00 1.000000e+00]]\n",
      "[[2.5476783e-19 0.0000000e+00 1.0000000e+00]]\n",
      "[[2.9341864e-32 0.0000000e+00 1.0000000e+00]]\n",
      "[[3.4364154e-14 0.0000000e+00 1.0000000e+00]]\n",
      "[[1.0000000e+00 0.0000000e+00 3.4017567e-09]]\n",
      "[[1.0000000e+00 0.0000000e+00 4.8843063e-26]]\n",
      "[[6.6238776e-10 0.0000000e+00 1.0000000e+00]]\n",
      "[[2.4790654e-17 0.0000000e+00 1.0000000e+00]]\n",
      "[[1.0000000e+00 0.0000000e+00 1.3782562e-09]]\n",
      "[[3.127813e-32 0.000000e+00 1.000000e+00]]\n",
      "[[2.2724531e-07 1.4392726e-18 9.9999976e-01]]\n",
      "[[1.000000e+00 0.000000e+00 4.523778e-12]]\n",
      "[[1.0000000e+00 0.0000000e+00 1.8681386e-23]]\n",
      "[[1.0000000e+00 0.0000000e+00 1.0884432e-28]]\n",
      "[[1.00000e+00 0.00000e+00 7.17092e-33]]\n",
      "[[1.000000e+00 0.000000e+00 4.494406e-09]]\n",
      "[[3.321725e-17 0.000000e+00 1.000000e+00]]\n",
      "[[1.0000000e+00 0.0000000e+00 4.8440733e-13]]\n",
      "[[5.2701656e-05 0.0000000e+00 9.9994731e-01]]\n",
      "[[0.00150026 0.         0.99849975]]\n",
      "[[1.6043775e-16 0.0000000e+00 1.0000000e+00]]\n",
      "[[1.0000000e+00 0.0000000e+00 1.3663044e-10]]\n",
      "[[1.000000e+00 0.000000e+00 9.101233e-12]]\n",
      "[[9.999919e-01 0.000000e+00 8.137581e-06]]\n",
      "[[1.0000000e+00 0.0000000e+00 2.8905326e-35]]\n",
      "[[1.0000000e+00 0.0000000e+00 1.0320431e-14]]\n",
      "[[1. 0. 0.]]\n",
      "[[1.0000000e+00 0.0000000e+00 4.9806775e-34]]\n",
      "[[1.000000e+00 0.000000e+00 4.315164e-15]]\n",
      "[[4.2503548e-04 0.0000000e+00 9.9957496e-01]]\n",
      "[[9.9999988e-01 0.0000000e+00 1.2431538e-07]]\n",
      "[[2.2078566e-16 0.0000000e+00 1.0000000e+00]]\n",
      "[[9.99998e-01 0.00000e+00 2.00104e-06]]\n",
      "[[5.2522087e-24 0.0000000e+00 1.0000000e+00]]\n",
      "[[1.0000000e+00 0.0000000e+00 1.8040974e-09]]\n",
      "[[3.5445425e-12 0.0000000e+00 1.0000000e+00]]\n",
      "[[9.9914300e-01 0.0000000e+00 8.5701706e-04]]\n",
      "[[7.361805e-12 0.000000e+00 1.000000e+00]]\n",
      "[[9.9993408e-01 0.0000000e+00 6.5918495e-05]]\n",
      "[[1.0000000e+00 0.0000000e+00 2.0875957e-12]]\n",
      "[[1.5100164e-23 0.0000000e+00 1.0000000e+00]]\n",
      "[[4.3736771e-04 0.0000000e+00 9.9956256e-01]]\n",
      "[[1.000000e+00 0.000000e+00 9.910808e-13]]\n",
      "[[1.4436445e-14 0.0000000e+00 1.0000000e+00]]\n",
      "[[6.6583605e-09 0.0000000e+00 1.0000000e+00]]\n",
      "[[2.5550875e-17 0.0000000e+00 1.0000000e+00]]\n",
      "[[5.7932546e-31 0.0000000e+00 1.0000000e+00]]\n",
      "[[4.7566256e-10 0.0000000e+00 1.0000000e+00]]\n",
      "[[3.125287e-08 0.000000e+00 1.000000e+00]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e7df546397c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo_capture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mface\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mface_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mface\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e7df546397c1>\u001b[0m in \u001b[0;36mface_extractor\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Function detects faces and returns the cropped face\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# If no face detected, it returns the input image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfaces\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import random\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import image\n",
    "model = load_model('mlopstask1_vgg2.h5')\n",
    "\n",
    "# Loading the cascades\n",
    "face_cascade = cv2.CascadeClassifier('D://mlopsproject//haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n",
    "\n",
    "# Face Recognition with the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    _, frame = video_capture.read()\n",
    "    \n",
    "    face=face_extractor(frame)\n",
    "    if type(face) is np.ndarray:\n",
    "        face = cv2.resize(face, (224, 224))\n",
    "        im = Image.fromarray(face, 'RGB')\n",
    "        img_array = np.array(im)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        pred = model.predict(img_array)\n",
    "        print(pred)\n",
    "        name=\"None matching\"\n",
    "        \n",
    "        if(pred[0][0]):\n",
    "            name='Deepak'\n",
    "        \n",
    "            \n",
    "        cv2.putText(frame,name, (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame,\"No face found\", (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1)==13:\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
